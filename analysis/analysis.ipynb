{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allx2100/Desktop/Vanderbilt/Junior/CS 6363/contrastive-encoders/contrastive-encoders/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from data.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from scipy.linalg import sqrtm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = unpickle('../saved_models/mnist_cvae_new_2000.pkl')\n",
    "test_loader = unpickle('../data/contrastive_mnist_data_loaders.pkl')['test']\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [50, 70, 90, 110, 132]\n",
    "\n",
    "vae_model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(2, len(images), figsize=(10, 4))\n",
    "\n",
    "for idx, img_idx in enumerate(images):\n",
    "    test_image = test_loader.dataset[img_idx][0].to(device).view(1, 1, 28, 28)\n",
    "    \n",
    "    recon = vae_model(test_image)[0].view(1, 28, 28).cpu().detach()\n",
    "    x = test_image.view(1, 28, 28).cpu().detach()\n",
    "    \n",
    "    recon = recon * 0.5 + 0.5\n",
    "    x = x * 0.5 + 0.5\n",
    "    \n",
    "    axes[0, idx].imshow(x.permute(1, 2, 0), cmap='gray')\n",
    "    axes[0, idx].axis('off')\n",
    "    # axes[0, idx].set_title(f\"Orig {img_idx}\")\n",
    "    \n",
    "    axes[1, idx].imshow(recon.permute(1, 2, 0), cmap='gray')\n",
    "    axes[1, idx].axis('off')\n",
    "    # axes[1, idx].set_title(f\"Recon {img_idx}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "latents = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, lbls in test_loader:\n",
    "        images = images.to(device)\n",
    "        mu, _ = vae_model.encode(images)\n",
    "        # mu = vae_model.projection(mu)\n",
    "        latents.append(mu.cpu().numpy())\n",
    "        labels.append(lbls.cpu().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1)\n",
    "embedding = reducer.fit_transform(latents)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='tab10', s=10)\n",
    "plt.colorbar(scatter, ticks=range(10), label=\"Class label\")\n",
    "plt.title(\"UMAP Projection of CVAE Latent Space (Colored by MNIST Labels)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.randn((1, 10)).to(device)\n",
    "gen = vae_model.decode(random)[0].detach().cpu()\n",
    "gen = gen * 0.5 + 0.5\n",
    "\n",
    "\n",
    "plt.imshow(gen.permute(1, 2, 0), cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(latents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()\n",
    "inception.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorImageDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(dataloader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.cuda()\n",
    "            preds = inception(batch)\n",
    "            features.append(preds.cpu().numpy())\n",
    "    return np.concatenate(features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = unpickle('../saved_models/mnist_cvae_new_2_400.pkl')\n",
    "test_loader = unpickle('../data/mnist_data_loaders.pkl')['test']\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = test_loader.dataset.data.view(10000, 1, 28, 28)\n",
    "real_images = (real_images / 256) * 2 - 1\n",
    "real_images = real_images.repeat(1,3,1,1)\n",
    "real_images = F.interpolate(real_images, size=(299,299), mode='bilinear', align_corners=False)\n",
    "\n",
    "random = torch.randn((10000, 10)).to(device)\n",
    "gen_images = vae_model.decode(random).detach().cpu()\n",
    "gen_images = (gen_images * 0.5 + 0.5) * 2 - 1\n",
    "gen_images = gen_images.repeat(1,3,1,1)\n",
    "gen_images = F.interpolate(gen_images, size=(299,299), mode='bilinear', align_corners=False)\n",
    "\n",
    "real = TensorImageDataset(real_images)\n",
    "gen = TensorImageDataset(gen_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loader = DataLoader(real, batch_size=64, shuffle=False)\n",
    "fake_loader = DataLoader(gen, batch_size=64, shuffle=False)\n",
    "\n",
    "real_acts = get_activations(real_loader)\n",
    "fake_acts = get_activations(fake_loader)\n",
    "\n",
    "mu_real, sigma_real = np.mean(real_acts, axis=0), np.cov(real_acts, rowvar=False)\n",
    "mu_fake, sigma_fake = np.mean(fake_acts, axis=0), np.cov(fake_acts, rowvar=False)\n",
    "\n",
    "fid_score = calculate_fid(mu_real, sigma_real, mu_fake, sigma_fake)\n",
    "print(f\"FID Score: {fid_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive-encoders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
